{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#  Build a Large Language Model (LLM) from Scratch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "#  Chapter 0: Setup and Environment\n",
        "\n",
        "Before we start building, let's set up our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "PyTorch version: 2.9.1+cu128\n",
            "Environment ready!\n"
          ]
        }
      ],
      "source": [
        "# Import all necessary libraries\n",
        "import torch  # PyTorch: our deep learning framework\n",
        "import torch.nn as nn  # Neural network modules\n",
        "import torch.nn.functional as F  # Functional operations (activations, etc.)\n",
        "from torch.utils.data import Dataset, DataLoader  # Data loading utilities\n",
        "import numpy as np  # Numerical operations\n",
        "import matplotlib.pyplot as plt  # Plotting and visualization\n",
        "import math  # Mathematical operations\n",
        "from tqdm import tqdm  # Progress bars\n",
        "import re  # Regular expressions for text processing\n",
        "\n",
        "# Check if GPU is available - purtroppo no nel mio caso, si va di big cpu e ram :D \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Environment ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "#  Chapter 1: Tokenization\n",
        "\n",
        "##  Theory\n",
        "\n",
        "**What is Tokenization?**\n",
        "\n",
        "Tokenization is the process of converting text into numbers that a neural network can understand.\n",
        "\n",
        "**Why do we need it?**\n",
        "- Neural networks work with numbers, not text\n",
        "- We need a systematic way to convert \"Hello world\" → [15496, 995]\n",
        "- Must be reversible: [15496, 995] → \"Hello world\"\n",
        "\n",
        "**Types of Tokenization:**\n",
        "\n",
        "1. **Character-level**: Each character is a token\n",
        "   - \"Hello\" → ['H', 'e', 'l', 'l', 'o']\n",
        "\n",
        "2. **Word-level**: Each word is a token\n",
        "   - \"Hello world\" → ['Hello', 'world']\n",
        "\n",
        "3. **Subword-level** (BPE, WordPiece): Best of both worlds\n",
        "   - \"tokenization\" → ['token', 'ization']\n",
        "\n",
        "\n",
        "##  Mathematics\n",
        "\n",
        "For a vocabulary of size V, each token is mapped to an integer:\n",
        "\n",
        "```\n",
        "tokenize: text → [token_1, token_2, ..., token_n]\n",
        "where each token_i ∈ {0, 1, 2, ..., V-1}\n",
        "```\n",
        "\n",
        "Example with V=50257 (GPT-2 vocabulary):\n",
        "- \"Hello\" → [15496]\n",
        "- \"world\" → [995]\n",
        "- \"Hello world\" → [15496, 995]\n",
        "\n",
        "##  Implementation\n",
        "\n",
        "Let's build a simple character-level tokenizer first to understand the concept:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CHARACTER-LEVEL TOKENIZATION EXAMPLE\n",
            "============================================================\n",
            "Vocabulary built: 22 unique characters\n",
            "Sample mappings: {' ': 0, '!': 1, ',': 2, '.': 3, 'H': 4, 'T': 5, 'W': 6, 'a': 7, 'd': 8, 'e': 9}\n",
            "\n",
            "Original text: Hello, World! This is a simple tokenizer.\n",
            "Encoded (first 20 tokens): [4, 9, 13, 13, 16, 2, 0, 6, 16, 18, 13, 8, 1, 0, 5, 10, 11, 19, 0, 11]\n",
            "Decoded text: Hello, World! This is a simple tokenizer.\n",
            "Match: True\n"
          ]
        }
      ],
      "source": [
        "class SimpleCharTokenizer:\n",
        "    \"\"\"\n",
        "    A simple character-level tokenizer for educational purposes.\n",
        "    \n",
        "    This tokenizer:\n",
        "    1. Builds a vocabulary from all unique characters in the text\n",
        "    2. Maps each character to a unique integer ID\n",
        "    3. Can encode text to IDs and decode IDs back to text\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        # char2idx: dictionary mapping characters to integer IDs\n",
        "        self.char2idx = {}\n",
        "        # idx2char: dictionary mapping integer IDs back to characters\n",
        "        self.idx2char = {}\n",
        "        # vocab_size: total number of unique characters\n",
        "        self.vocab_size = 0\n",
        "    \n",
        "    def build_vocabulary(self, text):\n",
        "        \"\"\"\n",
        "        Build vocabulary from text.\n",
        "        \n",
        "        Args:\n",
        "            text (str): Input text to build vocabulary from\n",
        "        \"\"\"\n",
        "        # Get all unique characters in the text and sort them\n",
        "        # Sorting ensures consistent ordering\n",
        "        unique_chars = sorted(set(text))\n",
        "        \n",
        "        # Enumerate creates (index, char) pairs starting from 0\n",
        "        for idx, char in enumerate(unique_chars):\n",
        "            self.char2idx[char] = idx  # Map character to index\n",
        "            self.idx2char[idx] = char  # Map index back to character\n",
        "        \n",
        "        # Store vocabulary size for easy access\n",
        "        self.vocab_size = len(unique_chars)\n",
        "        \n",
        "        print(f\"Vocabulary built: {self.vocab_size} unique characters\")\n",
        "        print(f\"Sample mappings: {dict(list(self.char2idx.items())[:10])}\")\n",
        "    \n",
        "    def encode(self, text):\n",
        "        \"\"\"\n",
        "        Convert text to list of token IDs.\n",
        "        \n",
        "        Args:\n",
        "            text (str): Text to encode\n",
        "            \n",
        "        Returns:\n",
        "            list: List of integer token IDs\n",
        "        \"\"\"\n",
        "        # For each character in text, look up its ID in char2idx\n",
        "        # get(char, 0) returns 0 if character is not in vocabulary\n",
        "        return [self.char2idx.get(char, 0) for char in text]\n",
        "    \n",
        "    def decode(self, ids):\n",
        "        \"\"\"\n",
        "        Convert list of token IDs back to text.\n",
        "        \n",
        "        Args:\n",
        "            ids (list): List of integer token IDs\n",
        "            \n",
        "        Returns:\n",
        "            str: Decoded text\n",
        "        \"\"\"\n",
        "        # For each ID, look up its character in idx2char\n",
        "        # get(id, '?') returns '?' if ID is not in vocabulary\n",
        "        return ''.join([self.idx2char.get(id, '?') for id in ids])\n",
        "\n",
        "\n",
        "# Example: Let's test our tokenizer\n",
        "print(\"=\" * 60)\n",
        "print(\"CHARACTER-LEVEL TOKENIZATION EXAMPLE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Sample text\n",
        "sample_text = \"Hello, World! This is a simple tokenizer.\"\n",
        "\n",
        "# Create and train tokenizer\n",
        "tokenizer = SimpleCharTokenizer()\n",
        "tokenizer.build_vocabulary(sample_text)\n",
        "\n",
        "# Encode text to IDs\n",
        "encoded = tokenizer.encode(sample_text)\n",
        "print(f\"\\nOriginal text: {sample_text}\")\n",
        "print(f\"Encoded (first 20 tokens): {encoded[:20]}\")\n",
        "\n",
        "# Decode IDs back to text\n",
        "decoded = tokenizer.decode(encoded)\n",
        "print(f\"Decoded text: {decoded}\")\n",
        "print(f\"Match: {sample_text == decoded}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  GPT-2 Tokenizer\n",
        "\n",
        "For real LLMs, we use more sophisticated tokenizers like GPT-2's BPE tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "GPT-2 TOKENIZATION EXAMPLE\n",
            "============================================================\n",
            "Original: Hello, World! Let's tokenize this text.\n",
            "Tokens: [15496, 11, 2159, 0, 3914, 338, 11241, 1096, 428, 2420, 13]\n",
            "Decoded: Hello, World! Let's tokenize this text.\n",
            "Vocabulary size: ~50,000 tokens\n",
            "\n",
            "Subword example: 'tokenization' →\n",
            "  [30001] = 'token'\n",
            "  [1634] = 'ization'\n"
          ]
        }
      ],
      "source": [
        "# Try to import tiktoken (GPT-2 tokenizer)\n",
        "# If not available, we'll use our simple tokenizer\n",
        "try:\n",
        "    import tiktoken\n",
        "    \n",
        "    # Get GPT-2 tokenizer\n",
        "    gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "    \n",
        "    # Test it\n",
        "    text = \"Hello, World! Let's tokenize this text.\"\n",
        "    tokens = gpt2_tokenizer.encode(text)\n",
        "    decoded = gpt2_tokenizer.decode(tokens)\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"GPT-2 TOKENIZATION EXAMPLE\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Original: {text}\")\n",
        "    print(f\"Tokens: {tokens}\")\n",
        "    print(f\"Decoded: {decoded}\")\n",
        "    print(f\"Vocabulary size: ~50,000 tokens\")\n",
        "    \n",
        "    # Show how subword tokenization works\n",
        "    word = \"tokenization\"\n",
        "    word_tokens = gpt2_tokenizer.encode(word)\n",
        "    print(f\"\\nSubword example: '{word}' →\")\n",
        "    for token_id in word_tokens:\n",
        "        token_str = gpt2_tokenizer.decode([token_id])\n",
        "        print(f\"  [{token_id}] = '{token_str}'\")\n",
        "    \n",
        "    USE_GPT2_TOKENIZER = True\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"\\ntiktoken not available, using simple tokenizer\")\n",
        "    USE_GPT2_TOKENIZER = False\n",
        "    gpt2_tokenizer = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "#  Chapter 2: Embeddings\n",
        "\n",
        "##  Theory\n",
        "\n",
        "**What are Embeddings?**\n",
        "\n",
        "Embeddings convert discrete tokens (integers) into continuous vectors that capture semantic meaning.\n",
        "\n",
        "**Why embeddings?**\n",
        "- Token IDs are just numbers: 15496, 995\n",
        "- No semantic relationship: ID 15496 isn't \"close\" to 15497 in meaning\n",
        "- Embeddings place similar words close in vector space:\n",
        "  - \"king\" and \"queen\" → similar vectors\n",
        "  - \"king\" and \"pizza\" → different vectors\n",
        "\n",
        "**Types of Embeddings in GPT:**\n",
        "\n",
        "1. **Token Embeddings**: Encode word meaning\n",
        "2. **Positional Embeddings**: Encode word position in sequence\n",
        "\n",
        "##  Mathematics\n",
        "\n",
        "**Token Embedding**:\n",
        "```\n",
        "E_token: {0, 1, ..., V-1} → ℝ^d\n",
        "\n",
        "For token ID i:\n",
        "e_i = E_token[i] ∈ ℝ^d\n",
        "```\n",
        "\n",
        "Where:\n",
        "- V = vocabulary size (e.g., 50257)\n",
        "- d = embedding dimension (e.g., 768)\n",
        "\n",
        "**Positional Embedding**:\n",
        "```\n",
        "E_pos: {0, 1, ..., L-1} → ℝ^d\n",
        "\n",
        "For position j:\n",
        "p_j = E_pos[j] ∈ ℝ^d\n",
        "```\n",
        "\n",
        "Where L = max sequence length (e.g., 1024)\n",
        "\n",
        "**Final Embedding**:\n",
        "```\n",
        "x_j = e_(token_j) + p_j\n",
        "```\n",
        "\n",
        "##  Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "EMBEDDING EXAMPLE\n",
            "============================================================\n",
            "\n",
            "Input tokens shape: torch.Size([2, 5])\n",
            "Sample tokens:\n",
            "tensor([[ 5, 12, 23, 45, 67],\n",
            "        [ 8, 15, 34, 56, 78]])\n",
            "\n",
            "Token embeddings shape: torch.Size([2, 5, 8])\n",
            "First token embedding (first 4 dims):\n",
            "tensor([-1.5576,  0.9956, -0.8798, -0.6011], grad_fn=<SliceBackward0>)\n",
            "\n",
            "Final embeddings shape: torch.Size([2, 5, 8])\n",
            "Final embedding of first token (first 4 dims):\n",
            "tensor([-0.6363,  1.5239, -0.8880, -2.0505], grad_fn=<SliceBackward0>)\n",
            "\n",
            "Position 0 vs Position 4 (same token=5):\n",
            "Token 5 at pos 0: tensor([-0.6363,  1.5239, -0.8880, -2.0505], grad_fn=<SliceBackward0>)\n",
            "Token 5 at pos 4: tensor([0.0335, 0.5328, 1.5409, 1.1594], grad_fn=<SliceBackward0>)\n",
            "Note: Different due to positional embeddings!\n"
          ]
        }
      ],
      "source": [
        "class TokenEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Token Embedding Layer.\n",
        "    \n",
        "    Converts token IDs to dense vector representations.\n",
        "    This is essentially a lookup table where each token ID\n",
        "    corresponds to a learnable vector.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        \"\"\"\n",
        "        Initialize token embedding.\n",
        "        \n",
        "        Args:\n",
        "            vocab_size (int): Size of vocabulary (number of unique tokens)\n",
        "            embed_dim (int): Dimension of embedding vectors\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "        # Create embedding matrix of shape (vocab_size, embed_dim)\n",
        "        # nn.Embedding is a simple lookup table that stores embeddings\n",
        "        # of a fixed dictionary and size.\n",
        "        # \n",
        "        # Example with vocab_size=10, embed_dim=4:\n",
        "        # token_embedding.weight = tensor of shape (10, 4)\n",
        "        # token_embedding(torch.tensor([0, 5, 3])) returns shape (3, 4)\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        \n",
        "        self.embed_dim = embed_dim\n",
        "    \n",
        "    def forward(self, token_ids):\n",
        "        \"\"\"\n",
        "        Convert token IDs to embeddings.\n",
        "        \n",
        "        Args:\n",
        "            token_ids: Tensor of shape (batch_size, seq_len)\n",
        "                      containing token IDs\n",
        "        \n",
        "        Returns:\n",
        "            Tensor of shape (batch_size, seq_len, embed_dim)\n",
        "            containing token embeddings\n",
        "        \"\"\"\n",
        "        # Simply look up embeddings for each token ID\n",
        "        # Input: (batch_size, seq_len) - integers\n",
        "        # Output: (batch_size, seq_len, embed_dim) - floats\n",
        "        return self.token_embedding(token_ids)\n",
        "\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Positional Embedding Layer.\n",
        "    \n",
        "    Adds position information to token embeddings.\n",
        "    Since transformers have no inherent notion of order,\n",
        "    we must explicitly encode position information.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, max_seq_len, embed_dim):\n",
        "        \"\"\"\n",
        "        Initialize positional embedding.\n",
        "        \n",
        "        Args:\n",
        "            max_seq_len (int): Maximum sequence length\n",
        "            embed_dim (int): Dimension of embedding vectors\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "        # Create positional embedding matrix\n",
        "        # Shape: (max_seq_len, embed_dim)\n",
        "        # Each position gets its own learnable vector\n",
        "        self.pos_embedding = nn.Embedding(max_seq_len, embed_dim)\n",
        "        \n",
        "        self.max_seq_len = max_seq_len\n",
        "    \n",
        "    def forward(self, token_embeddings):\n",
        "        \"\"\"\n",
        "        Add positional embeddings to token embeddings.\n",
        "        \n",
        "        Args:\n",
        "            token_embeddings: Tensor of shape (batch_size, seq_len, embed_dim)\n",
        "        \n",
        "        Returns:\n",
        "            Tensor of same shape with positional information added\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, embed_dim = token_embeddings.shape\n",
        "        \n",
        "        # Create position indices: [0, 1, 2, ..., seq_len-1]\n",
        "        # These are the same for all samples in the batch\n",
        "        positions = torch.arange(seq_len, device=token_embeddings.device)\n",
        "        \n",
        "        # Look up positional embeddings\n",
        "        # Shape: (seq_len, embed_dim)\n",
        "        pos_embeds = self.pos_embedding(positions)\n",
        "        \n",
        "        # Add positional embeddings to token embeddings\n",
        "        # Broadcasting: (batch_size, seq_len, embed_dim) + (seq_len, embed_dim)\n",
        "        # The (seq_len, embed_dim) tensor is broadcast across the batch dimension\n",
        "        return token_embeddings + pos_embeds\n",
        "\n",
        "\n",
        "#  Example: Let's see embeddings in action\n",
        "print(\"=\" * 60)\n",
        "print(\"EMBEDDING EXAMPLE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Hyperparameters\n",
        "vocab_size = 100  # Small vocabulary for demo\n",
        "embed_dim = 8     # Small embedding dimension for visualization\n",
        "max_seq_len = 10  # Maximum sequence length\n",
        "batch_size = 2    # Number of samples in batch\n",
        "\n",
        "# Create embedding layers\n",
        "token_emb = TokenEmbedding(vocab_size, embed_dim)\n",
        "pos_emb = PositionalEmbedding(max_seq_len, embed_dim)\n",
        "\n",
        "# Create sample data: batch of token IDs\n",
        "# Shape: (batch_size, seq_len)\n",
        "sample_tokens = torch.tensor([\n",
        "    [5, 12, 23, 45, 67],  # Sample 1: 5 tokens\n",
        "    [8, 15, 34, 56, 78],  # Sample 2: 5 tokens\n",
        "])\n",
        "\n",
        "print(f\"\\nInput tokens shape: {sample_tokens.shape}\")\n",
        "print(f\"Sample tokens:\\n{sample_tokens}\")\n",
        "\n",
        "# Get token embeddings\n",
        "token_embeddings = token_emb(sample_tokens)\n",
        "print(f\"\\nToken embeddings shape: {token_embeddings.shape}\")\n",
        "print(f\"First token embedding (first 4 dims):\\n{token_embeddings[0, 0, :4]}\")\n",
        "\n",
        "# Add positional embeddings\n",
        "final_embeddings = pos_emb(token_embeddings)\n",
        "print(f\"\\nFinal embeddings shape: {final_embeddings.shape}\")\n",
        "print(f\"Final embedding of first token (first 4 dims):\\n{final_embeddings[0, 0, :4]}\")\n",
        "\n",
        "# Visualize the difference\n",
        "print(f\"\\nPosition 0 vs Position 4 (same token=5):\")\n",
        "print(f\"Token 5 at pos 0: {final_embeddings[0, 0, :4]}\")\n",
        "print(f\"Token 5 at pos 4: {final_embeddings[0, 4, :4]}\")\n",
        "print(\"Note: Different due to positional embeddings!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "#  Chapter 3: Self-Attention Mechanism\n",
        "\n",
        "##  Theory\n",
        "\n",
        "**What is Self-Attention?**\n",
        "\n",
        "Self-attention allows each token to look at all other tokens in the sequence and decide which ones are important for understanding its meaning.\n",
        "\n",
        "**Real-world analogy:**\n",
        "Consider the sentence: \"The bank of the river\"\n",
        "- The word \"bank\" is ambiguous\n",
        "- By attending to \"river\", the model understands \"bank\" means riverbank, not financial bank\n",
        "\n",
        "**Key Ideas:**\n",
        "1. **Query (Q)**: \"What am I looking for?\"\n",
        "2. **Key (K)**: \"What do I offer?\"\n",
        "3. **Value (V)**: \"What information do I contain?\"\n",
        "\n",
        "Each token creates Q, K, V vectors. Attention computes:\n",
        "- How much does my Query match each Key? (relevance)\n",
        "- Weighted sum of Values based on relevance\n",
        "\n",
        "##  Mathematics\n",
        "\n",
        "**Step-by-step attention computation:**\n",
        "\n",
        "Given input X ∈ ℝ^(n×d):\n",
        "\n",
        "**1. Project to Q, K, V**:\n",
        "```\n",
        "Q = XW_Q    where W_Q ∈ ℝ^(d×d_k)\n",
        "K = XW_K    where W_K ∈ ℝ^(d×d_k)\n",
        "V = XW_V    where W_V ∈ ℝ^(d×d_v)\n",
        "```\n",
        "\n",
        "**2. Compute attention scores**:\n",
        "```\n",
        "S = QK^T / √d_k\n",
        "```\n",
        "- QK^T: dot product measures similarity\n",
        "- √d_k: scaling factor (prevents large values)\n",
        "\n",
        "**3. Apply softmax**:\n",
        "```\n",
        "A = softmax(S)\n",
        "```\n",
        "- Converts scores to probabilities\n",
        "- Each row sums to 1\n",
        "\n",
        "**4. Compute weighted sum**:\n",
        "```\n",
        "Output = AV\n",
        "```\n",
        "\n",
        "**Complete formula**:\n",
        "```\n",
        "Attention(Q, K, V) = softmax(QK^T / √d_k)V\n",
        "```\n",
        "\n",
        "##  Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "SELF-ATTENTION EXAMPLE\n",
            "============================================================\n",
            "\n",
            "Input shape: torch.Size([1, 4, 8])\n",
            "Input (showing first 4 dims per token):\n",
            "  Token 0: [1.0, 1.0, 1.0, 1.0]\n",
            "  Token 1: [-1.0, -1.0, -1.0, -1.0]\n",
            "  Token 2: [2.0, 2.0, 2.0, 2.0]\n",
            "  Token 3: [0.0, 0.0, 0.0, 0.0]\n",
            "\n",
            "Output shape: torch.Size([1, 4, 8])\n",
            "Attention weights shape: torch.Size([1, 4, 4])\n",
            "\n",
            "Attention Matrix (token i → token j):\n",
            "         Tok 0  Tok 1  Tok 2  Tok 3  \n",
            "Token 0: 0.238 0.285 0.217 0.260 \n",
            "Token 1: 0.260 0.217 0.285 0.238 \n",
            "Token 2: 0.224 0.322 0.186 0.268 \n",
            "Token 3: 0.250 0.250 0.250 0.250 \n",
            "\n",
            " Each row sums to 1.0: tensor([1.0000, 1.0000, 1.0000, 1.0000], grad_fn=<SumBackward1>)\n"
          ]
        }
      ],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Self-Attention mechanism.\n",
        "    \n",
        "    This is the core component that allows tokens to attend to each other.\n",
        "    Each token creates Query, Key, Value vectors and computes\n",
        "    attention weights to aggregate information from other tokens.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, embed_dim, head_dim):\n",
        "        \"\"\"\n",
        "        Initialize self-attention.\n",
        "        \n",
        "        Args:\n",
        "            embed_dim (int): Dimension of input embeddings\n",
        "            head_dim (int): Dimension of Query, Key, Value projections\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "        # Linear projections for Query, Key, Value\n",
        "        # These are learnable weight matrices\n",
        "        self.W_query = nn.Linear(embed_dim, head_dim, bias=False)\n",
        "        self.W_key = nn.Linear(embed_dim, head_dim, bias=False)\n",
        "        self.W_value = nn.Linear(embed_dim, head_dim, bias=False)\n",
        "        \n",
        "        # Store dimensions for later use\n",
        "        self.head_dim = head_dim\n",
        "        # Scaling factor: 1/sqrt(d_k)\n",
        "        self.scale = head_dim ** -0.5\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Compute self-attention.\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, seq_len, embed_dim)\n",
        "        \n",
        "        Returns:\n",
        "            Tensor of shape (batch_size, seq_len, head_dim)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, embed_dim = x.shape\n",
        "        \n",
        "        # Step 1: Project to Query, Key, Value\n",
        "        # Each is shape (batch_size, seq_len, head_dim)\n",
        "        Q = self.W_query(x)  # What each token is looking for\n",
        "        K = self.W_key(x)    # What each token offers\n",
        "        V = self.W_value(x)  # The actual information each token contains\n",
        "        \n",
        "        # Step 2: Compute attention scores\n",
        "        # Q @ K^T: (batch_size, seq_len, head_dim) @ (batch_size, head_dim, seq_len)\n",
        "        # Result: (batch_size, seq_len, seq_len)\n",
        "        # Element (i,j) = how much token i attends to token j\n",
        "        attention_scores = torch.matmul(Q, K.transpose(-2, -1))\n",
        "        \n",
        "        # Step 3: Scale attention scores\n",
        "        # Divide by sqrt(head_dim) to prevent softmax saturation\n",
        "        # Larger head_dim → larger dot products → more extreme softmax\n",
        "        attention_scores = attention_scores * self.scale\n",
        "        \n",
        "        # Step 4: Apply softmax to get attention weights\n",
        "        # Each row becomes a probability distribution (sums to 1)\n",
        "        # attention_weights[i, j] = probability that token i attends to token j\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "        \n",
        "        # Step 5: Weighted sum of Values\n",
        "        # attention_weights @ V: (batch_size, seq_len, seq_len) @ (batch_size, seq_len, head_dim)\n",
        "        # Result: (batch_size, seq_len, head_dim)\n",
        "        # Each token's output = weighted combination of all Value vectors\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "        \n",
        "        return output, attention_weights\n",
        "\n",
        "\n",
        "#  Example: Visualize attention\n",
        "print(\"=\" * 60)\n",
        "print(\"SELF-ATTENTION EXAMPLE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Small example for visualization\n",
        "embed_dim = 8\n",
        "head_dim = 8\n",
        "seq_len = 4\n",
        "batch_size = 1\n",
        "\n",
        "# Create attention module\n",
        "attention = SelfAttention(embed_dim, head_dim)\n",
        "\n",
        "# Create sample input (could be embeddings from previous layer)\n",
        "# Let's use some structured input to see attention patterns\n",
        "x = torch.randn(batch_size, seq_len, embed_dim)\n",
        "x[0, 0, :] = 1.0   # Token 0: all 1s\n",
        "x[0, 1, :] = -1.0  # Token 1: all -1s\n",
        "x[0, 2, :] = 2.0   # Token 2: all 2s\n",
        "x[0, 3, :] = 0.0   # Token 3: all 0s\n",
        "\n",
        "print(f\"\\nInput shape: {x.shape}\")\n",
        "print(f\"Input (showing first 4 dims per token):\")\n",
        "for i in range(seq_len):\n",
        "    print(f\"  Token {i}: {x[0, i, :4].tolist()}\")\n",
        "\n",
        "# Forward pass\n",
        "output, attn_weights = attention(x)\n",
        "\n",
        "print(f\"\\nOutput shape: {output.shape}\")\n",
        "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
        "\n",
        "# Visualize attention matrix\n",
        "print(f\"\\nAttention Matrix (token i → token j):\")\n",
        "print(f\"{'':8} \", end=\"\")\n",
        "for j in range(seq_len):\n",
        "    print(f\"Tok{j:2}  \", end=\"\")\n",
        "print()\n",
        "for i in range(seq_len):\n",
        "    print(f\"Token {i}: \", end=\"\")\n",
        "    for j in range(seq_len):\n",
        "        print(f\"{attn_weights[0, i, j].item():.3f} \", end=\"\")\n",
        "    print()\n",
        "\n",
        "print(f\"\\n Each row sums to 1.0: {attn_weights[0].sum(dim=1)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "#  Chapter 4: Multi-Head Attention\n",
        "\n",
        "##  Theory\n",
        "\n",
        "**Why Multiple Heads?**\n",
        "\n",
        "Single attention focuses on one type of relationship. Multiple heads can capture different relationships simultaneously:\n",
        "\n",
        "- **Head 1**: Syntactic relationships (subject-verb)\n",
        "- **Head 2**: Semantic relationships (synonyms)\n",
        "- **Head 3**: Positional relationships (nearby words)\n",
        "- etc.\n",
        "\n",
        "**Real-world analogy:**\n",
        "Like having multiple experts analyze the same text:\n",
        "- Grammar expert\n",
        "- Meaning expert\n",
        "- Context expert\n",
        "\n",
        "Their combined insights are more powerful!\n",
        "\n",
        "##  Mathematics\n",
        "\n",
        "**Multi-Head Attention with h heads:**\n",
        "\n",
        "For each head i ∈ {1, ..., h}:\n",
        "```\n",
        "head_i = Attention(XW^Q_i, XW^K_i, XW^V_i)\n",
        "```\n",
        "\n",
        "**Concatenate and project:**\n",
        "```\n",
        "MultiHead(X) = Concat(head_1, ..., head_h)W^O\n",
        "\n",
        "where:\n",
        "- W^Q_i, W^K_i, W^V_i ∈ ℝ^(d×d_k)  (per-head projections)\n",
        "- d_k = d/h  (head dimension)\n",
        "- W^O ∈ ℝ^(d×d)  (output projection)\n",
        "```\n",
        "\n",
        "**Key insight**: \n",
        "- Split d dimensions into h heads of d/h dimensions each\n",
        "- Each head learns different patterns\n",
        "- Combine via learned projection\n",
        "\n",
        "##  Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "MULTI-HEAD ATTENTION EXAMPLE\n",
            "============================================================\n",
            "\n",
            "Input shape: torch.Size([1, 6, 64])\n",
            "Number of heads: 4\n",
            "Dimensions per head: 16\n",
            "\n",
            "Output shape: torch.Size([1, 6, 64])\n",
            "Attention weights shape: torch.Size([1, 4, 6, 6])\n",
            "\n",
            "Attention patterns (token i → token j) for each head:\n",
            "\n",
            "--- Head 1 ---\n",
            "Token 0: 0.147 0.113 0.161 0.175 \n",
            "Token 1: 0.152 0.224 0.109 0.186 \n",
            "Token 2: 0.107 0.162 0.145 0.112 \n",
            "Token 3: 0.132 0.157 0.190 0.184 \n",
            "\n",
            "--- Head 2 ---\n",
            "Token 0: 0.249 0.149 0.131 0.124 \n",
            "Token 1: 0.117 0.214 0.150 0.147 \n",
            "Token 2: 0.208 0.135 0.162 0.157 \n",
            "Token 3: 0.242 0.101 0.192 0.157 \n",
            "\n",
            "--- Head 3 ---\n",
            "Token 0: 0.171 0.125 0.136 0.189 \n",
            "Token 1: 0.122 0.292 0.162 0.142 \n",
            "Token 2: 0.230 0.133 0.161 0.145 \n",
            "Token 3: 0.179 0.173 0.133 0.136 \n",
            "\n",
            "--- Head 4 ---\n",
            "Token 0: 0.171 0.104 0.223 0.150 \n",
            "Token 1: 0.146 0.175 0.259 0.084 \n",
            "Token 2: 0.122 0.227 0.168 0.128 \n",
            "Token 3: 0.197 0.180 0.155 0.191 \n",
            "\n",
            " Different heads learn different attention patterns!\n"
          ]
        }
      ],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Head Self-Attention.\n",
        "    \n",
        "    Runs multiple attention mechanisms in parallel, each focusing\n",
        "    on different aspects of the relationships between tokens.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        \"\"\"\n",
        "        Initialize multi-head attention.\n",
        "        \n",
        "        Args:\n",
        "            embed_dim (int): Total embedding dimension\n",
        "            num_heads (int): Number of attention heads\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "        # Ensure embedding dimension is divisible by number of heads\n",
        "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
        "        \n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        # Each head processes embed_dim/num_heads dimensions\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "        \n",
        "        # Single linear layer for all heads (more efficient than separate layers)\n",
        "        # We'll split this into multiple heads after projection\n",
        "        self.W_query = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.W_key = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.W_value = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        \n",
        "        # Output projection to combine all heads\n",
        "        self.W_out = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "    \n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass through multi-head attention.\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor (batch_size, seq_len, embed_dim)\n",
        "            mask: Optional attention mask (batch_size, 1, seq_len, seq_len)\n",
        "        \n",
        "        Returns:\n",
        "            output: Tensor (batch_size, seq_len, embed_dim)\n",
        "            attention_weights: Tensor (batch_size, num_heads, seq_len, seq_len)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, embed_dim = x.shape\n",
        "        \n",
        "        # Step 1: Project to Q, K, V\n",
        "        # Shape: (batch_size, seq_len, embed_dim)\n",
        "        Q = self.W_query(x)\n",
        "        K = self.W_key(x)\n",
        "        V = self.W_value(x)\n",
        "        \n",
        "        # Step 2: Split into multiple heads\n",
        "        # Reshape from (batch_size, seq_len, embed_dim)\n",
        "        # to (batch_size, seq_len, num_heads, head_dim)\n",
        "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        \n",
        "        # Transpose to (batch_size, num_heads, seq_len, head_dim)\n",
        "        # This allows us to process all heads in parallel\n",
        "        Q = Q.transpose(1, 2)\n",
        "        K = K.transpose(1, 2)\n",
        "        V = V.transpose(1, 2)\n",
        "        \n",
        "        # Step 3: Compute attention for all heads simultaneously\n",
        "        # Q @ K^T: (batch_size, num_heads, seq_len, head_dim) @ \n",
        "        #          (batch_size, num_heads, head_dim, seq_len)\n",
        "        # Result: (batch_size, num_heads, seq_len, seq_len)\n",
        "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
        "        \n",
        "        # Step 4: Apply mask (if provided)\n",
        "        if mask is not None:\n",
        "            # Set masked positions to large negative value\n",
        "            # so softmax makes them near zero\n",
        "            attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
        "        \n",
        "        # Step 5: Softmax to get attention weights\n",
        "        # Shape: (batch_size, num_heads, seq_len, seq_len)\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "        \n",
        "        # Step 6: Apply attention to Values\n",
        "        # (batch_size, num_heads, seq_len, seq_len) @ \n",
        "        # (batch_size, num_heads, seq_len, head_dim)\n",
        "        # Result: (batch_size, num_heads, seq_len, head_dim)\n",
        "        context = torch.matmul(attention_weights, V)\n",
        "        \n",
        "        # Step 7: Concatenate heads\n",
        "        # Transpose back: (batch_size, seq_len, num_heads, head_dim)\n",
        "        context = context.transpose(1, 2)\n",
        "        # Reshape to (batch_size, seq_len, embed_dim)\n",
        "        context = context.contiguous().view(batch_size, seq_len, embed_dim)\n",
        "        \n",
        "        # Step 8: Final output projection\n",
        "        # Allows heads to communicate and combine their information\n",
        "        output = self.W_out(context)\n",
        "        \n",
        "        return output, attention_weights\n",
        "\n",
        "\n",
        "#  Example: Multi-head attention in action\n",
        "print(\"=\" * 60)\n",
        "print(\"MULTI-HEAD ATTENTION EXAMPLE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Parameters\n",
        "embed_dim = 64\n",
        "num_heads = 4  # Split into 4 heads of 16 dimensions each\n",
        "seq_len = 6\n",
        "batch_size = 1\n",
        "\n",
        "# Create multi-head attention\n",
        "mha = MultiHeadAttention(embed_dim, num_heads)\n",
        "\n",
        "# Create input\n",
        "x = torch.randn(batch_size, seq_len, embed_dim)\n",
        "\n",
        "print(f\"\\nInput shape: {x.shape}\")\n",
        "print(f\"Number of heads: {num_heads}\")\n",
        "print(f\"Dimensions per head: {embed_dim // num_heads}\")\n",
        "\n",
        "# Forward pass\n",
        "output, attn_weights = mha(x)\n",
        "\n",
        "print(f\"\\nOutput shape: {output.shape}\")\n",
        "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
        "\n",
        "# Visualize attention patterns for each head\n",
        "print(f\"\\nAttention patterns (token i → token j) for each head:\")\n",
        "for head in range(num_heads):\n",
        "    print(f\"\\n--- Head {head + 1} ---\")\n",
        "    for i in range(min(4, seq_len)):  # Show first 4 tokens\n",
        "        print(f\"Token {i}: \", end=\"\")\n",
        "        for j in range(min(4, seq_len)):\n",
        "            print(f\"{attn_weights[0, head, i, j].item():.3f} \", end=\"\")\n",
        "        print()\n",
        "\n",
        "print(f\"\\n Different heads learn different attention patterns!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "#  Chapter 5: Feed-Forward Network\n",
        "\n",
        "##  Theory\n",
        "\n",
        "**What is the Feed-Forward Network?**\n",
        "\n",
        "After attention aggregates information, the feed-forward network (FFN) processes each position independently with non-linear transformations.\n",
        "\n",
        "**Purpose:**\n",
        "- Add non-linearity (attention is mostly linear)\n",
        "- Increase model capacity\n",
        "- Transform representations\n",
        "\n",
        "**Architecture:**\n",
        "- Two linear layers with activation in between\n",
        "- Applied identically to each position\n",
        "- No interaction between positions (unlike attention)\n",
        "\n",
        "##  Mathematics\n",
        "\n",
        "**Feed-Forward Network:**\n",
        "```\n",
        "FFN(x) = W_2 · ReLU(W_1 · x + b_1) + b_2\n",
        "\n",
        "where:\n",
        "- W_1 ∈ ℝ^(d×4d): First projection (expand)\n",
        "- W_2 ∈ ℝ^(4d×d): Second projection (compress)\n",
        "- ReLU(x) = max(0, x): Non-linear activation\n",
        "```\n",
        "\n",
        "**Expansion factor:**\n",
        "- Hidden dimension = 4 × embed_dim (standard in GPT)\n",
        "- Expand to high dimension, then compress back\n",
        "- Allows complex transformations\n",
        "\n",
        "**Alternative activations:**\n",
        "- GELU (Gaussian Error Linear Unit): smoother than ReLU\n",
        "- Used in GPT-2 and GPT-3\n",
        "\n",
        "##  Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "FEED-FORWARD NETWORK EXAMPLE\n",
            "============================================================\n",
            "\n",
            "Input shape: torch.Size([1, 5, 64])\n",
            "Hidden dimension: 256 (4x expansion)\n",
            "Output shape: torch.Size([1, 5, 64])\n",
            "\n",
            "Input statistics:\n",
            "  Mean: 0.0117, Std: 0.9556\n",
            "Output statistics:\n",
            "  Mean: 0.0205, Std: 0.2186\n",
            "\n",
            "Sample values (position 0, first 4 dims):\n",
            "  Input:  tensor([ 0.4203, -1.3742, -0.6026, -1.6693])\n",
            "  Output: tensor([ 0.1387, -0.1539,  0.1921,  0.2566], grad_fn=<SliceBackward0>)\n"
          ]
        }
      ],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    Position-wise Feed-Forward Network.\n",
        "    \n",
        "    Two-layer neural network with ReLU activation applied\n",
        "    independently to each position in the sequence.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, embed_dim, hidden_dim, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Initialize feed-forward network.\n",
        "        \n",
        "        Args:\n",
        "            embed_dim (int): Input/output dimension\n",
        "            hidden_dim (int): Hidden layer dimension (typically 4 * embed_dim)\n",
        "            dropout (float): Dropout probability\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "        # First linear layer: expand dimensions\n",
        "        # (batch_size, seq_len, embed_dim) → (batch_size, seq_len, hidden_dim)\n",
        "        self.linear1 = nn.Linear(embed_dim, hidden_dim)\n",
        "        \n",
        "        # Second linear layer: compress back to embed_dim\n",
        "        # (batch_size, seq_len, hidden_dim) → (batch_size, seq_len, embed_dim)\n",
        "        self.linear2 = nn.Linear(hidden_dim, embed_dim)\n",
        "        \n",
        "        # Dropout for regularization\n",
        "        # Randomly zeros some elements during training\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through feed-forward network.\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor (batch_size, seq_len, embed_dim)\n",
        "        \n",
        "        Returns:\n",
        "            Tensor of same shape (batch_size, seq_len, embed_dim)\n",
        "        \"\"\"\n",
        "        # Step 1: First linear transformation (expand)\n",
        "        # Shape: (batch_size, seq_len, hidden_dim)\n",
        "        x = self.linear1(x)\n",
        "        \n",
        "        # Step 2: Apply non-linear activation (ReLU)\n",
        "        # ReLU(x) = max(0, x): sets negative values to 0\n",
        "        # This adds non-linearity to the model\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        # Step 3: Apply dropout\n",
        "        # During training: randomly set some activations to 0\n",
        "        # During inference: does nothing\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        # Step 4: Second linear transformation (compress)\n",
        "        # Shape: (batch_size, seq_len, embed_dim)\n",
        "        x = self.linear2(x)\n",
        "        \n",
        "        # Step 5: Apply dropout again\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "# Alternative: GELU activation (used in GPT-2/3)\n",
        "class FeedForwardGELU(nn.Module):\n",
        "    \"\"\"\n",
        "    Feed-Forward Network with GELU activation.\n",
        "    \n",
        "    GELU (Gaussian Error Linear Unit) is a smoother alternative to ReLU.\n",
        "    It's used in GPT-2 and GPT-3.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, embed_dim, hidden_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(embed_dim, hidden_dim)\n",
        "        self.linear2 = nn.Linear(hidden_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Apply GELU instead of ReLU\n",
        "        # GELU(x) ≈ x * Φ(x) where Φ is the CDF of standard normal\n",
        "        # Smoother than ReLU, better gradient flow\n",
        "        x = F.gelu(self.linear1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "#  Example: Feed-forward transformation\n",
        "print(\"=\" * 60)\n",
        "print(\"FEED-FORWARD NETWORK EXAMPLE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Parameters\n",
        "embed_dim = 64\n",
        "hidden_dim = 256  # 4x expansion (standard)\n",
        "seq_len = 5\n",
        "batch_size = 1\n",
        "\n",
        "# Create FFN\n",
        "ffn = FeedForward(embed_dim, hidden_dim, dropout=0.0)  # No dropout for demo\n",
        "\n",
        "# Create input\n",
        "x = torch.randn(batch_size, seq_len, embed_dim)\n",
        "\n",
        "print(f\"\\nInput shape: {x.shape}\")\n",
        "print(f\"Hidden dimension: {hidden_dim} ({hidden_dim // embed_dim}x expansion)\")\n",
        "\n",
        "# Forward pass\n",
        "output = ffn(x)\n",
        "\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"\\nInput statistics:\")\n",
        "print(f\"  Mean: {x.mean():.4f}, Std: {x.std():.4f}\")\n",
        "print(f\"Output statistics:\")\n",
        "print(f\"  Mean: {output.mean():.4f}, Std: {output.std():.4f}\")\n",
        "\n",
        "# Show transformation effect\n",
        "print(f\"\\nSample values (position 0, first 4 dims):\")\n",
        "print(f\"  Input:  {x[0, 0, :4]}\")\n",
        "print(f\"  Output: {output[0, 0, :4]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "#  Chapter 6: Transformer Block\n",
        "\n",
        "##  Theory\n",
        "\n",
        "**What is a Transformer Block?**\n",
        "\n",
        "A Transformer Block combines all previous components with:\n",
        "- **Residual connections**: Skip connections for better gradient flow\n",
        "- **Layer normalization**: Stabilizes training\n",
        "- **Attention + FFN**: Core transformations\n",
        "\n",
        "**Architecture:**\n",
        "```\n",
        "Input → [Add & Norm → Attention] → [Add & Norm → FFN] → Output\n",
        "         ↑________________|           ↑______________|\n",
        "         Residual Connection         Residual Connection\n",
        "```\n",
        "\n",
        "**Why Residual Connections?**\n",
        "- Allow gradients to flow directly through network\n",
        "- Enable training of very deep models (100+ layers)\n",
        "- Help preserve information from earlier layers\n",
        "\n",
        "**Why Layer Normalization?**\n",
        "- Normalizes across feature dimension (not batch)\n",
        "- Stabilizes training\n",
        "- Allows higher learning rates\n",
        "\n",
        "##  Mathematics\n",
        "\n",
        "**Transformer Block:**\n",
        "```\n",
        "# Sub-layer 1: Multi-head attention\n",
        "x' = LayerNorm(x + MultiHeadAttention(x))\n",
        "\n",
        "# Sub-layer 2: Feed-forward\n",
        "output = LayerNorm(x' + FeedForward(x'))\n",
        "```\n",
        "\n",
        "**Layer Normalization:**\n",
        "```\n",
        "LayerNorm(x) = γ · (x - μ) / (σ + ε) + β\n",
        "\n",
        "where:\n",
        "- μ = mean(x) over feature dimension\n",
        "- σ = std(x) over feature dimension\n",
        "- γ, β: learnable parameters\n",
        "- ε: small constant for numerical stability\n",
        "```\n",
        "\n",
        "##  Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "TRANSFORMER BLOCK EXAMPLE\n",
            "============================================================\n",
            "\n",
            "Input shape: torch.Size([1, 8, 64])\n",
            "Parameters:\n",
            "  - Embedding dimension: 64\n",
            "  - Attention heads: 4\n",
            "  - FFN hidden dim: 256\n",
            "\n",
            "Output shape: torch.Size([1, 8, 64])\n",
            "Input/Output match: True\n",
            "\n",
            "Information flow:\n",
            "  Input norm: 22.4049\n",
            "  Output norm: 23.1439\n",
            "  Difference norm: 5.8884\n",
            "\n",
            " Residual connections preserve and transform information!\n"
          ]
        }
      ],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Single Transformer Block.\n",
        "    \n",
        "    Combines multi-head attention and feed-forward network\n",
        "    with residual connections and layer normalization.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, embed_dim, num_heads, hidden_dim, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Initialize transformer block.\n",
        "        \n",
        "        Args:\n",
        "            embed_dim (int): Embedding dimension\n",
        "            num_heads (int): Number of attention heads\n",
        "            hidden_dim (int): Hidden dimension for FFN\n",
        "            dropout (float): Dropout probability\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "        # Multi-head attention layer\n",
        "        self.attention = MultiHeadAttention(embed_dim, num_heads)\n",
        "        \n",
        "        # Feed-forward network\n",
        "        self.feed_forward = FeedForwardGELU(embed_dim, hidden_dim, dropout)\n",
        "        \n",
        "        # Layer normalization layers\n",
        "        # One before attention, one before feed-forward\n",
        "        self.ln1 = nn.LayerNorm(embed_dim)\n",
        "        self.ln2 = nn.LayerNorm(embed_dim)\n",
        "        \n",
        "        # Dropout for residual connections\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass through transformer block.\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor (batch_size, seq_len, embed_dim)\n",
        "            mask: Optional attention mask\n",
        "        \n",
        "        Returns:\n",
        "            Output tensor (batch_size, seq_len, embed_dim)\n",
        "        \"\"\"\n",
        "        # Sub-layer 1: Multi-head attention with residual connection\n",
        "        # \n",
        "        # Step 1: Layer norm BEFORE attention (pre-norm architecture)\n",
        "        # This is more stable than post-norm (original transformer)\n",
        "        normed = self.ln1(x)\n",
        "        \n",
        "        # Step 2: Apply multi-head attention\n",
        "        attention_output, _ = self.attention(normed, mask)\n",
        "        \n",
        "        # Step 3: Apply dropout to attention output\n",
        "        attention_output = self.dropout(attention_output)\n",
        "        \n",
        "        # Step 4: Add residual connection (x + attention_output)\n",
        "        # This allows gradients to flow directly through the block\n",
        "        x = x + attention_output\n",
        "        \n",
        "        # Sub-layer 2: Feed-forward with residual connection\n",
        "        #\n",
        "        # Step 5: Layer norm before feed-forward\n",
        "        normed = self.ln2(x)\n",
        "        \n",
        "        # Step 6: Apply feed-forward network\n",
        "        ff_output = self.feed_forward(normed)\n",
        "        \n",
        "        # Step 7: Add residual connection\n",
        "        x = x + ff_output\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "#  Example: Transformer block\n",
        "print(\"=\" * 60)\n",
        "print(\"TRANSFORMER BLOCK EXAMPLE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Parameters\n",
        "embed_dim = 64\n",
        "num_heads = 4\n",
        "hidden_dim = 256\n",
        "seq_len = 8\n",
        "batch_size = 1\n",
        "\n",
        "# Create transformer block\n",
        "block = TransformerBlock(embed_dim, num_heads, hidden_dim, dropout=0.0)\n",
        "\n",
        "# Create input\n",
        "x = torch.randn(batch_size, seq_len, embed_dim)\n",
        "\n",
        "print(f\"\\nInput shape: {x.shape}\")\n",
        "print(f\"Parameters:\")\n",
        "print(f\"  - Embedding dimension: {embed_dim}\")\n",
        "print(f\"  - Attention heads: {num_heads}\")\n",
        "print(f\"  - FFN hidden dim: {hidden_dim}\")\n",
        "\n",
        "# Forward pass\n",
        "output = block(x)\n",
        "\n",
        "print(f\"\\nOutput shape: {output.shape}\")\n",
        "print(f\"Input/Output match: {x.shape == output.shape}\")\n",
        "\n",
        "# Check that information flows through\n",
        "print(f\"\\nInformation flow:\")\n",
        "print(f\"  Input norm: {x.norm():.4f}\")\n",
        "print(f\"  Output norm: {output.norm():.4f}\")\n",
        "print(f\"  Difference norm: {(output - x).norm():.4f}\")\n",
        "print(f\"\\n Residual connections preserve and transform information!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "#  Chapter 7: Complete GPT Model\n",
        "\n",
        "## Theory\n",
        "\n",
        "**GPT Architecture:**\n",
        "\n",
        "GPT (Generative Pre-trained Transformer) stacks multiple transformer blocks:\n",
        "\n",
        "```\n",
        "Input Text\n",
        "    ↓\n",
        "Tokenization\n",
        "    ↓\n",
        "Token Embeddings + Positional Embeddings\n",
        "    ↓\n",
        "Transformer Block 1\n",
        "    ↓\n",
        "Transformer Block 2\n",
        "    ↓\n",
        "    ...\n",
        "    ↓\n",
        "Transformer Block N\n",
        "    ↓\n",
        "Layer Normalization\n",
        "    ↓\n",
        "Linear (to vocabulary)\n",
        "    ↓\n",
        "Softmax\n",
        "    ↓\n",
        "Probability Distribution over Next Token\n",
        "```\n",
        "\n",
        "**Key Features:**\n",
        "1. **Causal masking**: Can only attend to previous tokens\n",
        "2. **Autoregressive**: Generates one token at a time\n",
        "3. **Pre-training**: Trained to predict next token\n",
        "4. **Transfer learning**: Can be fine-tuned for specific tasks\n",
        "\n",
        "**GPT-2 Sizes:**\n",
        "- Small: 117M parameters, 12 layers\n",
        "- Medium: 345M parameters, 24 layers\n",
        "- Large: 762M parameters, 36 layers\n",
        "- XL: 1.5B parameters, 48 layers\n",
        "\n",
        "##  Mathematics\n",
        "\n",
        "**Complete Forward Pass:**\n",
        "\n",
        "Given input tokens [t₁, t₂, ..., tₙ]:\n",
        "\n",
        "1. **Embed**: E = TokenEmbed(t) + PosEmbed(position)\n",
        "2. **Transform**: For each layer i: E → TransformerBlock_i(E)\n",
        "3. **Project**: logits = Linear(LayerNorm(E))\n",
        "4. **Probability**: P(next_token) = Softmax(logits)\n",
        "\n",
        "**Causal Mask:**\n",
        "```\n",
        "Mask[i,j] = 1 if j ≤ i else 0\n",
        "\n",
        "Prevents token i from attending to future tokens j > i\n",
        "```\n",
        "\n",
        "##  Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "COMPLETE GPT MODEL EXAMPLE\n",
            "============================================================\n",
            "\n",
            "Model Configuration:\n",
            "  vocab_size: 50257\n",
            "  embed_dim: 256\n",
            "  num_heads: 8\n",
            "  num_layers: 6\n",
            "  max_seq_len: 128\n",
            "  hidden_dim: 1024\n",
            "  dropout: 0.1\n",
            "\n",
            "Model Statistics:\n",
            "  Total parameters: 30,497,280\n",
            "  Trainable parameters: 30,497,280\n",
            "  Model size: ~116.3 MB (float32)\n",
            "\n",
            "Test Forward Pass:\n",
            "  Input shape: torch.Size([2, 10])\n",
            "  Output shape: torch.Size([2, 10, 50257])\n",
            "  Logits range: [-1.66, 1.65]\n",
            "\n",
            "Test Generation:\n",
            "  Start tokens: [[1, 2, 3]]\n",
            "  Generated: [[1, 2, 3, 13097, 6053, 17890, 20288, 37903]]\n",
            "\n",
            " Complete GPT model is ready!\n"
          ]
        }
      ],
      "source": [
        "class GPT(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete GPT Model.\n",
        "    \n",
        "    Implements the full architecture with embeddings, transformer blocks,\n",
        "    and output projection.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, \n",
        "                 max_seq_len, hidden_dim, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Initialize GPT model.\n",
        "        \n",
        "        Args:\n",
        "            vocab_size (int): Size of vocabulary\n",
        "            embed_dim (int): Embedding dimension\n",
        "            num_heads (int): Number of attention heads\n",
        "            num_layers (int): Number of transformer blocks\n",
        "            max_seq_len (int): Maximum sequence length\n",
        "            hidden_dim (int): Hidden dimension for FFN\n",
        "            dropout (float): Dropout probability\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "        # Store configuration\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.max_seq_len = max_seq_len\n",
        "        \n",
        "        # Token embeddings: convert token IDs to vectors\n",
        "        # Shape: (vocab_size, embed_dim)\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        \n",
        "        # Positional embeddings: add position information\n",
        "        # Shape: (max_seq_len, embed_dim)\n",
        "        self.position_embedding = nn.Embedding(max_seq_len, embed_dim)\n",
        "        \n",
        "        # Dropout on embeddings\n",
        "        self.embed_dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        # Stack of transformer blocks\n",
        "        # Each block has same architecture but different parameters\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, hidden_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        \n",
        "        # Final layer normalization\n",
        "        self.ln_final = nn.LayerNorm(embed_dim)\n",
        "        \n",
        "        # Output projection: embed_dim → vocab_size\n",
        "        # Predicts logits for each token in vocabulary\n",
        "        self.head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
        "        \n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "    \n",
        "    def _init_weights(self, module):\n",
        "        \"\"\"\n",
        "        Initialize weights using Xavier/Glorot initialization.\n",
        "        This helps with training stability.\n",
        "        \"\"\"\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            # Initialize with normal distribution, std=0.02\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "                # Initialize bias to zero\n",
        "                module.bias.data.zero_()\n",
        "    \n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        Forward pass through GPT model.\n",
        "        \n",
        "        Args:\n",
        "            input_ids: Token IDs, shape (batch_size, seq_len)\n",
        "        \n",
        "        Returns:\n",
        "            logits: Predictions for next token, shape (batch_size, seq_len, vocab_size)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "        \n",
        "        # Step 1: Get token embeddings\n",
        "        # Shape: (batch_size, seq_len, embed_dim)\n",
        "        token_embeds = self.token_embedding(input_ids)\n",
        "        \n",
        "        # Step 2: Get positional embeddings\n",
        "        # Create position indices: [0, 1, 2, ..., seq_len-1]\n",
        "        positions = torch.arange(seq_len, device=input_ids.device)\n",
        "        # Shape: (seq_len, embed_dim)\n",
        "        pos_embeds = self.position_embedding(positions)\n",
        "        \n",
        "        # Step 3: Combine embeddings\n",
        "        # Broadcasting: (batch_size, seq_len, embed_dim) + (seq_len, embed_dim)\n",
        "        x = token_embeds + pos_embeds\n",
        "        \n",
        "        # Step 4: Apply dropout to embeddings\n",
        "        x = self.embed_dropout(x)\n",
        "        \n",
        "        # Step 5: Create causal mask for attention\n",
        "        # Shape: (1, 1, seq_len, seq_len)\n",
        "        # Lower triangular matrix: can only attend to previous positions\n",
        "        causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=input_ids.device))\n",
        "        causal_mask = causal_mask.view(1, 1, seq_len, seq_len)\n",
        "        \n",
        "        # Step 6: Pass through transformer blocks\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x, mask=causal_mask)\n",
        "        \n",
        "        # Step 7: Final layer normalization\n",
        "        x = self.ln_final(x)\n",
        "        \n",
        "        # Step 8: Project to vocabulary\n",
        "        # Shape: (batch_size, seq_len, vocab_size)\n",
        "        logits = self.head(x)\n",
        "        \n",
        "        return logits\n",
        "    \n",
        "    def generate(self, input_ids, max_new_tokens=50, temperature=1.0):\n",
        "        \"\"\"\n",
        "        Generate text autoregressively.\n",
        "        \n",
        "        Args:\n",
        "            input_ids: Starting tokens (batch_size, seq_len)\n",
        "            max_new_tokens: Number of tokens to generate\n",
        "            temperature: Controls randomness (higher = more random)\n",
        "        \n",
        "        Returns:\n",
        "            Generated token IDs (batch_size, seq_len + max_new_tokens)\n",
        "        \"\"\"\n",
        "        # Set model to evaluation mode\n",
        "        self.eval()\n",
        "        \n",
        "        # Start with input tokens\n",
        "        generated = input_ids.clone()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for _ in range(max_new_tokens):\n",
        "                # Get predictions for current sequence\n",
        "                # Truncate if sequence is too long\n",
        "                current_seq = generated[:, -self.max_seq_len:]\n",
        "                logits = self(current_seq)\n",
        "                \n",
        "                # Get logits for last position\n",
        "                last_logits = logits[:, -1, :] / temperature\n",
        "                \n",
        "                # Convert to probabilities\n",
        "                probs = F.softmax(last_logits, dim=-1)\n",
        "                \n",
        "                # Sample next token\n",
        "                next_token = torch.multinomial(probs, num_samples=1)\n",
        "                \n",
        "                # Append to sequence\n",
        "                generated = torch.cat([generated, next_token], dim=1)\n",
        "        \n",
        "        return generated\n",
        "\n",
        "\n",
        "#  Example: Create and test GPT model\n",
        "print(\"=\" * 60)\n",
        "print(\"COMPLETE GPT MODEL EXAMPLE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Small GPT configuration (like GPT-2 Small but smaller)\n",
        "config = {\n",
        "    'vocab_size': 50257,      # GPT-2 vocabulary\n",
        "    'embed_dim': 256,         # Embedding dimension (GPT-2: 768)\n",
        "    'num_heads': 8,           # Number of heads (GPT-2: 12)\n",
        "    'num_layers': 6,          # Number of layers (GPT-2: 12)\n",
        "    'max_seq_len': 128,       # Max sequence length (GPT-2: 1024)\n",
        "    'hidden_dim': 1024,       # FFN hidden dim (GPT-2: 3072)\n",
        "    'dropout': 0.1\n",
        "}\n",
        "\n",
        "print(\"\\nModel Configuration:\")\n",
        "for key, value in config.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# Create model\n",
        "model = GPT(**config)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\nModel Statistics:\")\n",
        "print(f\"  Total parameters: {total_params:,}\")\n",
        "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"  Model size: ~{total_params * 4 / 1024**2:.1f} MB (float32)\")\n",
        "\n",
        "# Test forward pass\n",
        "batch_size = 2\n",
        "seq_len = 10\n",
        "input_ids = torch.randint(0, config['vocab_size'], (batch_size, seq_len))\n",
        "\n",
        "print(f\"\\nTest Forward Pass:\")\n",
        "print(f\"  Input shape: {input_ids.shape}\")\n",
        "\n",
        "# Forward pass\n",
        "logits = model(input_ids)\n",
        "\n",
        "print(f\"  Output shape: {logits.shape}\")\n",
        "print(f\"  Logits range: [{logits.min():.2f}, {logits.max():.2f}]\")\n",
        "\n",
        "# Test generation\n",
        "print(f\"\\nTest Generation:\")\n",
        "start_tokens = torch.tensor([[1, 2, 3]])  # Start with 3 tokens\n",
        "generated = model.generate(start_tokens, max_new_tokens=5, temperature=1.0)\n",
        "print(f\"  Start tokens: {start_tokens.tolist()}\")\n",
        "print(f\"  Generated: {generated.tolist()}\")\n",
        "print(f\"\\n Complete GPT model is ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "#  Chapter 8: Training GPT\n",
        "\n",
        "##  Theory\n",
        "\n",
        "**Pre-training Objective:**\n",
        "\n",
        "GPT is trained with **next-token prediction** (language modeling):\n",
        "\n",
        "Given tokens [t₁, t₂, ..., tₙ], predict each next token:\n",
        "- Input: [t₁, t₂, ..., tₙ₋₁]\n",
        "- Target: [t₂, t₃, ..., tₙ]\n",
        "\n",
        "**Why this works:**\n",
        "- Model learns language patterns\n",
        "- Captures syntax, semantics, and world knowledge\n",
        "- Enables zero-shot transfer to many tasks\n",
        "\n",
        "**Training Process:**\n",
        "1. **Data**: Large corpus of text (books, websites, etc.)\n",
        "2. **Loss**: Cross-entropy between predictions and targets\n",
        "3. **Optimization**: AdamW optimizer with learning rate scheduling\n",
        "4. **Evaluation**: Perplexity on validation set\n",
        "\n",
        "##  Mathematics\n",
        "\n",
        "**Cross-Entropy Loss:**\n",
        "```\n",
        "Loss = -∑ log P(token_i | token_<i)\n",
        "\n",
        "where P(token_i | token_<i) is the predicted probability\n",
        "of the correct next token given all previous tokens.\n",
        "```\n",
        "\n",
        "**Perplexity:**\n",
        "```\n",
        "PPL = exp(Loss)\n",
        "\n",
        "Lower perplexity = better model\n",
        "- Perfect model: PPL = 1\n",
        "- Random guessing: PPL = vocab_size\n",
        "```\n",
        "\n",
        "**AdamW Optimizer:**\n",
        "```\n",
        "m_t = β₁ m_{t-1} + (1-β₁) g_t\n",
        "v_t = β₂ v_{t-1} + (1-β₂) g_t²\n",
        "θ_t = θ_{t-1} - α (m_t / √v_t + ε) - λθ_{t-1}\n",
        "\n",
        "where:\n",
        "- g_t: gradient\n",
        "- m_t: first moment (momentum)\n",
        "- v_t: second moment (variance)\n",
        "- α: learning rate\n",
        "- λ: weight decay\n",
        "```\n",
        "\n",
        "##  Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "TRAINING EXAMPLE\n",
            "============================================================\n",
            "\n",
            "Sample text length: 1990 characters\n",
            "Dataset created:\n",
            "  Text length: 1,990 characters\n",
            "  Vocabulary size: 31\n",
            "  Number of tokens: 1,990\n",
            "\n",
            "Dataset split:\n",
            "  Train samples: 1566\n",
            "  Val samples: 392\n",
            "\n",
            "Model config:\n",
            "  vocab_size: 31\n",
            "  embed_dim: 64\n",
            "  num_heads: 4\n",
            "  num_layers: 2\n",
            "  max_seq_len: 32\n",
            "  hidden_dim: 128\n",
            "  dropout: 0.1\n",
            "\n",
            "Total parameters: 72,576\n",
            "\n",
            "Starting training...\n",
            "============================================================\n",
            "  Batch 100/392, Loss: 2.4073\n",
            "  Batch 200/392, Loss: 1.8437\n",
            "  Batch 300/392, Loss: 1.5727\n",
            "\n",
            "Epoch 1/3\n",
            "  Train Loss: 2.0275, Perplexity: 7.59\n",
            "  Val Loss: 1.1689, Perplexity: 3.22\n",
            "------------------------------------------------------------\n",
            "  Batch 100/392, Loss: 1.1990\n",
            "  Batch 200/392, Loss: 0.8554\n",
            "  Batch 300/392, Loss: 0.6822\n",
            "\n",
            "Epoch 2/3\n",
            "  Train Loss: 0.9321, Perplexity: 2.54\n",
            "  Val Loss: 0.5241, Perplexity: 1.69\n",
            "------------------------------------------------------------\n",
            "  Batch 100/392, Loss: 0.6262\n",
            "  Batch 200/392, Loss: 0.4806\n",
            "  Batch 300/392, Loss: 0.4551\n",
            "\n",
            "Epoch 3/3\n",
            "  Train Loss: 0.5034, Perplexity: 1.65\n",
            "  Val Loss: 0.2653, Perplexity: 1.30\n",
            "------------------------------------------------------------\n",
            "\n",
            " Training completed!\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1400x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final Results:\n",
            "  Train Loss: 0.5034\n",
            "  Val Loss: 0.2653\n",
            "  Train Perplexity: 1.65\n",
            "  Val Perplexity: 1.30\n"
          ]
        }
      ],
      "source": [
        "# Dataset class for GPT training\n",
        "class GPTDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for GPT pre-training.\n",
        "    \n",
        "    Creates (input, target) pairs from raw text where\n",
        "    target is shifted version of input (next token prediction).\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, text, tokenizer, max_seq_len):\n",
        "        \"\"\"\n",
        "        Initialize dataset.\n",
        "        \n",
        "        Args:\n",
        "            text (str): Raw text data\n",
        "            tokenizer: Tokenizer object with encode/decode methods\n",
        "            max_seq_len (int): Maximum sequence length\n",
        "        \"\"\"\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_seq_len = max_seq_len\n",
        "        \n",
        "        # Tokenize entire text\n",
        "        # For simplicity, use character-level tokenization\n",
        "        self.char_to_idx = {ch: i for i, ch in enumerate(sorted(set(text)))}\n",
        "        self.idx_to_char = {i: ch for ch, i in self.char_to_idx.items()}\n",
        "        self.vocab_size = len(self.char_to_idx)\n",
        "        \n",
        "        # Convert text to token IDs\n",
        "        self.token_ids = [self.char_to_idx[ch] for ch in text]\n",
        "        \n",
        "        print(f\"Dataset created:\")\n",
        "        print(f\"  Text length: {len(text):,} characters\")\n",
        "        print(f\"  Vocabulary size: {self.vocab_size}\")\n",
        "        print(f\"  Number of tokens: {len(self.token_ids):,}\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        # Number of possible sequences\n",
        "        return len(self.token_ids) - self.max_seq_len\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Get a training sample.\n",
        "        \n",
        "        Returns:\n",
        "            input_ids: Tokens [idx : idx+max_seq_len]\n",
        "            target_ids: Tokens [idx+1 : idx+max_seq_len+1]\n",
        "        \"\"\"\n",
        "        # Get chunk of token IDs\n",
        "        chunk = self.token_ids[idx : idx + self.max_seq_len + 1]\n",
        "        \n",
        "        # Split into input and target\n",
        "        # Input: all but last token\n",
        "        # Target: all but first token (shifted by 1)\n",
        "        input_ids = torch.tensor(chunk[:-1], dtype=torch.long)\n",
        "        target_ids = torch.tensor(chunk[1:], dtype=torch.long)\n",
        "        \n",
        "        return input_ids, target_ids\n",
        "\n",
        "\n",
        "# Training function\n",
        "def train_gpt(model, train_loader, val_loader, optimizer, device, num_epochs):\n",
        "    \"\"\"\n",
        "    Train GPT model.\n",
        "    \n",
        "    Args:\n",
        "        model: GPT model\n",
        "        train_loader: DataLoader for training\n",
        "        val_loader: DataLoader for validation\n",
        "        optimizer: Optimizer\n",
        "        device: Device to train on\n",
        "        num_epochs: Number of epochs\n",
        "    \n",
        "    Returns:\n",
        "        train_losses: List of training losses\n",
        "        val_losses: List of validation losses\n",
        "    \"\"\"\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        \n",
        "        for batch_idx, (input_ids, target_ids) in enumerate(train_loader):\n",
        "            # Move data to device\n",
        "            input_ids = input_ids.to(device)\n",
        "            target_ids = target_ids.to(device)\n",
        "            \n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass\n",
        "            # logits: (batch_size, seq_len, vocab_size)\n",
        "            logits = model(input_ids)\n",
        "            \n",
        "            # Calculate loss\n",
        "            # Flatten logits and targets for cross-entropy\n",
        "            # logits: (batch_size * seq_len, vocab_size)\n",
        "            # targets: (batch_size * seq_len)\n",
        "            loss = F.cross_entropy(\n",
        "                logits.view(-1, logits.size(-1)),\n",
        "                target_ids.view(-1)\n",
        "            )\n",
        "            \n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            \n",
        "            # Gradient clipping (prevent exploding gradients)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            \n",
        "            # Update weights\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_train_loss += loss.item()\n",
        "            \n",
        "            # Print progress every 100 batches\n",
        "            if (batch_idx + 1) % 100 == 0:\n",
        "                print(f\"  Batch {batch_idx + 1}/{len(train_loader)}, \"\n",
        "                      f\"Loss: {loss.item():.4f}\")\n",
        "        \n",
        "        # Calculate average training loss\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "        \n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for input_ids, target_ids in val_loader:\n",
        "                input_ids = input_ids.to(device)\n",
        "                target_ids = target_ids.to(device)\n",
        "                \n",
        "                logits = model(input_ids)\n",
        "                loss = F.cross_entropy(\n",
        "                    logits.view(-1, logits.size(-1)),\n",
        "                    target_ids.view(-1)\n",
        "                )\n",
        "                \n",
        "                total_val_loss += loss.item()\n",
        "        \n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        \n",
        "        # Calculate perplexity\n",
        "        train_ppl = torch.exp(torch.tensor(avg_train_loss))\n",
        "        val_ppl = torch.exp(torch.tensor(avg_val_loss))\n",
        "        \n",
        "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "        print(f\"  Train Loss: {avg_train_loss:.4f}, Perplexity: {train_ppl:.2f}\")\n",
        "        print(f\"  Val Loss: {avg_val_loss:.4f}, Perplexity: {val_ppl:.2f}\")\n",
        "        print(\"-\" * 60)\n",
        "    \n",
        "    return train_losses, val_losses\n",
        "\n",
        "\n",
        "#  Example: Train a tiny GPT\n",
        "print(\"=\" * 60)\n",
        "print(\"TRAINING EXAMPLE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Sample text (use a small text for demo)\n",
        "sample_text = \"\"\"\n",
        "To be, or not to be, that is the question:\n",
        "Whether 'tis nobler in the mind to suffer\n",
        "The slings and arrows of outrageous fortune,\n",
        "Or to take arms against a sea of troubles\n",
        "And by opposing end them.\n",
        "\"\"\" * 10  # Repeat to have enough data\n",
        "\n",
        "print(f\"\\nSample text length: {len(sample_text)} characters\")\n",
        "\n",
        "# Create dataset\n",
        "seq_len = 32\n",
        "dataset = GPTDataset(sample_text, None, seq_len)\n",
        "\n",
        "# Split into train/val (80/20)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "    dataset, [train_size, val_size]\n",
        ")\n",
        "\n",
        "# Create dataloaders\n",
        "batch_size = 4\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"\\nDataset split:\")\n",
        "print(f\"  Train samples: {len(train_dataset)}\")\n",
        "print(f\"  Val samples: {len(val_dataset)}\")\n",
        "\n",
        "# Create tiny GPT model\n",
        "tiny_config = {\n",
        "    'vocab_size': dataset.vocab_size,\n",
        "    'embed_dim': 64,\n",
        "    'num_heads': 4,\n",
        "    'num_layers': 2,\n",
        "    'max_seq_len': seq_len,\n",
        "    'hidden_dim': 128,\n",
        "    'dropout': 0.1\n",
        "}\n",
        "\n",
        "print(f\"\\nModel config:\")\n",
        "for k, v in tiny_config.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "model = GPT(**tiny_config).to(device)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"\\nTotal parameters: {total_params:,}\")\n",
        "\n",
        "# Setup optimizer\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=3e-4,           # Learning rate\n",
        "    weight_decay=0.01  # Weight decay for regularization\n",
        ")\n",
        "\n",
        "# Train for a few epochs\n",
        "print(f\"\\nStarting training...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "num_epochs = 3\n",
        "train_losses, val_losses = train_gpt(\n",
        "    model, train_loader, val_loader,\n",
        "    optimizer, device, num_epochs\n",
        ")\n",
        "\n",
        "print(f\"\\n Training completed!\")\n",
        "\n",
        "# Visualize training curves\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Loss curve\n",
        "ax1.plot(range(1, num_epochs+1), train_losses, 'o-', label='Train Loss')\n",
        "ax1.plot(range(1, num_epochs+1), val_losses, 's-', label='Val Loss')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('Training and Validation Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Perplexity curve\n",
        "train_ppls = [torch.exp(torch.tensor(l)).item() for l in train_losses]\n",
        "val_ppls = [torch.exp(torch.tensor(l)).item() for l in val_losses]\n",
        "ax2.plot(range(1, num_epochs+1), train_ppls, 'o-', label='Train PPL')\n",
        "ax2.plot(range(1, num_epochs+1), val_ppls, 's-', label='Val PPL')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Perplexity')\n",
        "ax2.set_title('Training and Validation Perplexity')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nFinal Results:\")\n",
        "print(f\"  Train Loss: {train_losses[-1]:.4f}\")\n",
        "print(f\"  Val Loss: {val_losses[-1]:.4f}\")\n",
        "print(f\"  Train Perplexity: {train_ppls[-1]:.2f}\")\n",
        "print(f\"  Val Perplexity: {val_ppls[-1]:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "#  Chapter 9: Text Generation Strategies\n",
        "\n",
        "##  Theory\n",
        "\n",
        "**Generation Methods:**\n",
        "\n",
        "1. **Greedy**: Always pick most probable token\n",
        "2. **Temperature Sampling**: Control randomness\n",
        "3. **Top-K**: Sample from K most likely tokens\n",
        "4. **Top-P (Nucleus)**: Sample from cumulative probability P\n",
        "\n",
        "##  Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "TEXT GENERATION EXAMPLES\n",
            "============================================================\n",
            "\n",
            "Start text: 'To be'\n",
            "\n",
            "1. GREEDY (deterministic):\n",
            "To be, or not to be, that is the question:\n",
            "Whether 'tis\n",
            "\n",
            "2. TEMPERATURE = 0.5 (conservative):\n",
            "To be, or not to be, that is the question:\n",
            "Whether 'tis\n",
            "\n",
            "3. TEMPERATURE = 1.0 (balanced):\n",
            "To be, or not to be, that is the nd to to ber\n",
            "Aslen opp\n",
            "\n",
            "4. TEMPERATURE = 1.5 (creative):\n",
            "To beles\n",
            "Ang ofd nd thforhe, the m.To kermgonf aAnstsli\n",
            "\n",
            "5. TOP-K = 5:\n",
            "To be, or ows of outranert tageous fortune,d take arrog\n",
            "\n",
            "6. TOP-P = 0.9 (nucleus):\n",
            "To be, or not to be, that is the question:\n",
            "Or to to ble\n",
            "\n",
            " Different strategies produce different styles of text!\n"
          ]
        }
      ],
      "source": [
        "def generate_text(model, dataset, start_text, max_new_tokens=100, \n",
        "                   method='temperature', temperature=1.0, top_k=None, top_p=None):\n",
        "    \"\"\"\n",
        "    Generate text using trained model.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained GPT model\n",
        "        dataset: Dataset (for char mapping)\n",
        "        start_text: Starting text\n",
        "        max_new_tokens: Number of tokens to generate\n",
        "        method: 'greedy', 'temperature', 'top_k', or 'top_p'\n",
        "        temperature: Temperature for sampling\n",
        "        top_k: K for top-k sampling\n",
        "        top_p: P for nucleus sampling\n",
        "    \n",
        "    Returns:\n",
        "        Generated text\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Encode start text\n",
        "    token_ids = [dataset.char_to_idx.get(ch, 0) for ch in start_text]\n",
        "    generated = torch.tensor([token_ids], dtype=torch.long).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Get predictions\n",
        "            logits = model(generated[:, -model.max_seq_len:])\n",
        "            next_logits = logits[:, -1, :]\n",
        "            \n",
        "            if method == 'greedy':\n",
        "                # Always pick most probable\n",
        "                next_token = torch.argmax(next_logits, dim=-1, keepdim=True)\n",
        "            \n",
        "            elif method == 'temperature':\n",
        "                # Scale by temperature and sample\n",
        "                scaled_logits = next_logits / temperature\n",
        "                probs = F.softmax(scaled_logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, num_samples=1)\n",
        "            \n",
        "            elif method == 'top_k':\n",
        "                # Keep only top k tokens\n",
        "                top_k_logits, top_k_indices = torch.topk(next_logits, top_k)\n",
        "                probs = F.softmax(top_k_logits / temperature, dim=-1)\n",
        "                next_token_idx = torch.multinomial(probs, num_samples=1)\n",
        "                next_token = top_k_indices.gather(-1, next_token_idx)\n",
        "            \n",
        "            elif method == 'top_p':\n",
        "                # Nucleus sampling\n",
        "                sorted_logits, sorted_indices = torch.sort(next_logits, descending=True)\n",
        "                probs = F.softmax(sorted_logits / temperature, dim=-1)\n",
        "                cumsum_probs = torch.cumsum(probs, dim=-1)\n",
        "                mask = cumsum_probs < top_p\n",
        "                mask[..., 0] = True  # Always keep at least one\n",
        "                filtered_logits = sorted_logits.clone()\n",
        "                filtered_logits[~mask] = float('-inf')\n",
        "                filtered_probs = F.softmax(filtered_logits / temperature, dim=-1)\n",
        "                next_token_idx = torch.multinomial(filtered_probs, num_samples=1)\n",
        "                next_token = sorted_indices.gather(-1, next_token_idx)\n",
        "            \n",
        "            generated = torch.cat([generated, next_token], dim=1)\n",
        "    \n",
        "    # Decode to text\n",
        "    token_ids = generated[0].tolist()\n",
        "    text = ''.join([dataset.idx_to_char.get(id, '?') for id in token_ids])\n",
        "    \n",
        "    return text\n",
        "\n",
        "\n",
        "#  Test different generation strategies\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TEXT GENERATION EXAMPLES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "start = \"To be\"\n",
        "\n",
        "print(f\"\\nStart text: '{start}'\\n\")\n",
        "\n",
        "# Greedy\n",
        "print(\"1. GREEDY (deterministic):\")\n",
        "print(generate_text(model, dataset, start, 50, method='greedy'))\n",
        "\n",
        "# Temperature sampling\n",
        "print(\"\\n2. TEMPERATURE = 0.5 (conservative):\")\n",
        "print(generate_text(model, dataset, start, 50, method='temperature', temperature=0.5))\n",
        "\n",
        "print(\"\\n3. TEMPERATURE = 1.0 (balanced):\")\n",
        "print(generate_text(model, dataset, start, 50, method='temperature', temperature=1.0))\n",
        "\n",
        "print(\"\\n4. TEMPERATURE = 1.5 (creative):\")\n",
        "print(generate_text(model, dataset, start, 50, method='temperature', temperature=1.5))\n",
        "\n",
        "# Top-K\n",
        "print(\"\\n5. TOP-K = 5:\")\n",
        "print(generate_text(model, dataset, start, 50, method='top_k', top_k=5, temperature=1.0))\n",
        "\n",
        "# Top-P\n",
        "print(\"\\n6. TOP-P = 0.9 (nucleus):\")\n",
        "print(generate_text(model, dataset, start, 50, method='top_p', top_p=0.9, temperature=1.0))\n",
        "\n",
        "print(f\"\\n Different strategies produce different styles of text!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "#  Chapter 10: Fine-tuning for Classification\n",
        "\n",
        "##  Theory\n",
        "\n",
        "**What is Fine-tuning?**\n",
        "\n",
        "After pretraining, we adapt (fine-tune) the model for specific tasks. For classification:\n",
        "- Start with pretrained GPT\n",
        "- Add classification head (linear layer)\n",
        "- Train on labeled data\n",
        "\n",
        "**Types of Fine-tuning:**\n",
        "\n",
        "1. **Classification**: Spam detection, sentiment analysis\n",
        "2. **Instruction Following**: Chatbots (next chapter!)\n",
        "3. **RLHF**: Alignment with human preferences\n",
        "\n",
        "**Why Fine-tune?**\n",
        "- Pretrained model has general language knowledge\n",
        "- Fine-tuning specializes it for your task\n",
        "- Much faster than training from scratch\n",
        "\n",
        "##  Mathematics\n",
        "\n",
        "**Classification Head:**\n",
        "```\n",
        "logits = Linear(pooled_hidden_state)\n",
        "where pooled_hidden_state = mean(hidden_states) or hidden_states[0]\n",
        "\n",
        "Loss = CrossEntropy(logits, labels)\n",
        "```\n",
        "\n",
        "**Training Strategy:**\n",
        "1. Freeze base model initially (optional)\n",
        "2. Train classification head\n",
        "3. Unfreeze and fine-tune entire model\n",
        "4. Use lower learning rate than pretraining\n",
        "\n",
        "##  Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CLASSIFICATION FINE-TUNING EXAMPLE\n",
            "============================================================\n",
            "\n",
            "Dataset created:\n",
            "  Total samples: 6\n",
            "  Train: 4\n",
            "  Val: 2\n",
            "\n",
            "Creating classification model...\n",
            "Model created with 140,674 parameters\n",
            "\n",
            "Starting fine-tuning...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/3: 100%|██████████| 2/2 [00:00<00:00, 46.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1:\n",
            "  Train Loss: 0.7476, Acc: 50.00%\n",
            "  Val Loss: 0.6960, Acc: 50.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/3: 100%|██████████| 2/2 [00:00<00:00, 38.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2:\n",
            "  Train Loss: 0.7175, Acc: 50.00%\n",
            "  Val Loss: 0.6991, Acc: 50.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/3: 100%|██████████| 2/2 [00:00<00:00, 40.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3:\n",
            "  Train Loss: 0.6948, Acc: 50.00%\n",
            "  Val Loss: 0.7009, Acc: 50.00%\n",
            "\n",
            " Fine-tuning completed!\n",
            "Final accuracy: 50.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Classification Dataset\n",
        "class ClassificationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for classification tasks (e.g., spam detection).\n",
        "    \n",
        "    Returns input_ids and labels for each sample.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            texts: List of text strings\n",
        "            labels: List of integer labels (0, 1, 2, ...)\n",
        "            tokenizer: Tokenizer with encode method\n",
        "            max_length: Maximum sequence length\n",
        "        \"\"\"\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        \n",
        "        # Tokenize (using simple char tokenizer for demo)\n",
        "        # In production, use proper tokenizer like tiktoken\n",
        "        token_ids = [ord(c) % 256 for c in text[:self.max_length]]\n",
        "        \n",
        "        # Pad to max_length\n",
        "        padding = [0] * (self.max_length - len(token_ids))\n",
        "        token_ids = token_ids + padding\n",
        "        \n",
        "        return {\n",
        "            'input_ids': torch.tensor(token_ids, dtype=torch.long),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "class GPTForClassification(nn.Module):\n",
        "    \"\"\"\n",
        "    GPT model adapted for classification.\n",
        "    \n",
        "    Architecture:\n",
        "    1. Use pretrained GPT as base\n",
        "    2. Pool hidden states (mean or [CLS])\n",
        "    3. Add classification head\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, base_model, num_classes=2, pooling='mean'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            base_model: Pretrained GPT model\n",
        "            num_classes: Number of output classes\n",
        "            pooling: 'mean', 'cls', or 'last'\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "        # Pretrained base model\n",
        "        self.base_model = base_model\n",
        "        self.pooling = pooling\n",
        "        \n",
        "        # Classification head\n",
        "        # Input: embed_dim from base model\n",
        "        # Output: num_classes logits\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(base_model.embed_dim, num_classes)\n",
        "    \n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        Forward pass for classification.\n",
        "        \n",
        "        Args:\n",
        "            input_ids: Token IDs (batch_size, seq_len)\n",
        "        \n",
        "        Returns:\n",
        "            logits: Class logits (batch_size, num_classes)\n",
        "        \"\"\"\n",
        "        # Get hidden states from base model\n",
        "        # Shape: (batch_size, seq_len, embed_dim)\n",
        "        logits = self.base_model(input_ids)\n",
        "        \n",
        "        # Extract final hidden states (before vocabulary projection)\n",
        "        # We need to get the hidden states before the final linear layer\n",
        "        # For simplicity in this demo, we'll use the embeddings\n",
        "        # In practice, you'd access intermediate layers\n",
        "        \n",
        "        # Pool across sequence dimension\n",
        "        if self.pooling == 'mean':\n",
        "            # Mean pooling: average across all tokens\n",
        "            pooled = logits.mean(dim=1)  # (batch_size, vocab_size)\n",
        "            # Take first embed_dim dimensions as proxy for hidden state\n",
        "            pooled = pooled[:, :self.base_model.embed_dim]\n",
        "        elif self.pooling == 'cls':\n",
        "            # Use first token (like BERT's [CLS])\n",
        "            pooled = logits[:, 0, :self.base_model.embed_dim]\n",
        "        else:  # 'last'\n",
        "            # Use last token\n",
        "            pooled = logits[:, -1, :self.base_model.embed_dim]\n",
        "        \n",
        "        # Apply dropout for regularization\n",
        "        pooled = self.dropout(pooled)\n",
        "        \n",
        "        # Classification head\n",
        "        # Shape: (batch_size, num_classes)\n",
        "        class_logits = self.classifier(pooled)\n",
        "        \n",
        "        return class_logits\n",
        "\n",
        "\n",
        "def train_classifier(model, train_loader, val_loader, optimizer, device, num_epochs):\n",
        "    \"\"\"\n",
        "    Training loop for classification.\n",
        "    \n",
        "    Uses CrossEntropyLoss and tracks accuracy.\n",
        "    \"\"\"\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        \n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            \n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass\n",
        "            logits = model(input_ids)\n",
        "            \n",
        "            # Calculate loss\n",
        "            loss = F.cross_entropy(logits, labels)\n",
        "            \n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            \n",
        "            # Clip gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            \n",
        "            # Update weights\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Track metrics\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(logits, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "        \n",
        "        # Calculate training metrics\n",
        "        train_loss = total_loss / len(train_loader)\n",
        "        train_acc = 100 * correct / total\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        \n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "                \n",
        "                logits = model(input_ids)\n",
        "                loss = F.cross_entropy(logits, labels)\n",
        "                \n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(logits, 1)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "        \n",
        "        val_loss = val_loss / len(val_loader)\n",
        "        val_acc = 100 * correct / total\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(val_acc)\n",
        "        \n",
        "        print(f\"\\nEpoch {epoch+1}:\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%\")\n",
        "        print(f\"  Val Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
        "    \n",
        "    return train_losses, val_losses, train_accs, val_accs\n",
        "\n",
        "\n",
        "#  Example: Train spam classifier\n",
        "print(\"=\" * 60)\n",
        "print(\"CLASSIFICATION FINE-TUNING EXAMPLE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create sample spam dataset\n",
        "spam_texts = [\n",
        "    \"Win free money now!\",\n",
        "    \"Meeting at 3pm\",\n",
        "    \"Click here for prizes\",\n",
        "    \"Project update attached\",\n",
        "    \"You won the lottery!\",\n",
        "    \"Dinner tomorrow?\",\n",
        "]\n",
        "spam_labels = [1, 0, 1, 0, 1, 0]  # 1=spam, 0=ham\n",
        "\n",
        "# Create simple char tokenizer\n",
        "class SimpleCharTokenizer:\n",
        "    def encode(self, text):\n",
        "        return [ord(c) % 256 for c in text]\n",
        "\n",
        "tokenizer = SimpleCharTokenizer()\n",
        "\n",
        "# Create dataset\n",
        "dataset = ClassificationDataset(spam_texts, spam_labels, tokenizer)\n",
        "\n",
        "# Split train/val\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
        "\n",
        "print(f\"\\nDataset created:\")\n",
        "print(f\"  Total samples: {len(dataset)}\")\n",
        "print(f\"  Train: {len(train_dataset)}\")\n",
        "print(f\"  Val: {len(val_dataset)}\")\n",
        "\n",
        "# Create model (reuse pretrained GPT if available, or create new)\n",
        "print(f\"\\nCreating classification model...\")\n",
        "\n",
        "# Use small GPT as base\n",
        "base_gpt = GPT(\n",
        "    vocab_size=256,  # Simple char vocab\n",
        "    embed_dim=64,\n",
        "    num_heads=4,\n",
        "    num_layers=2,\n",
        "    max_seq_len=128,\n",
        "    hidden_dim=256,\n",
        "    dropout=0.1\n",
        ").to(device)\n",
        "\n",
        "# Add classification head\n",
        "classifier_model = GPTForClassification(base_gpt, num_classes=2, pooling='mean').to(device)\n",
        "\n",
        "print(f\"Model created with {sum(p.numel() for p in classifier_model.parameters()):,} parameters\")\n",
        "\n",
        "# Setup optimizer (lower LR for fine-tuning)\n",
        "optimizer = torch.optim.AdamW(classifier_model.parameters(), lr=1e-4, weight_decay=0.01)\n",
        "\n",
        "# Train\n",
        "print(f\"\\nStarting fine-tuning...\")\n",
        "train_losses, val_losses, train_accs, val_accs = train_classifier(\n",
        "    classifier_model, train_loader, val_loader, optimizer, device, num_epochs=3\n",
        ")\n",
        "\n",
        "print(f\"\\n Fine-tuning completed!\")\n",
        "print(f\"Final accuracy: {val_accs[-1]:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "#  Chapter 11: Instruction Fine-tuning\n",
        "\n",
        "## Theory\n",
        "\n",
        "**What is Instruction Fine-tuning?**\n",
        "\n",
        "Train the model to follow instructions and generate helpful responses.\n",
        "\n",
        "**Dataset Format:**\n",
        "```json\n",
        "{\n",
        "  \"instruction\": \"What is Python?\",\n",
        "  \"input\": \"\",\n",
        "  \"output\": \"Python is a programming language...\"\n",
        "}\n",
        "```\n",
        "\n",
        "**Key Differences from Classification:**\n",
        "- Output: Text generation (not class label)\n",
        "- Loss: Next-token prediction on response\n",
        "- Evaluation: Response quality (harder to measure)\n",
        "\n",
        "**Applications:**\n",
        "- ChatGPT, Claude, Gemini\n",
        "- Question answering\n",
        "- Code generation\n",
        "- Creative writing\n",
        "\n",
        "##  Mathematics\n",
        "\n",
        "**Training Objective:**\n",
        "```\n",
        "Given: instruction + input\n",
        "Predict: output tokens autoregressively\n",
        "\n",
        "Loss = -∑ log P(output_token_i | instruction, input, output_<i)\n",
        "```\n",
        "\n",
        "**Template Format:**\n",
        "```\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Input:\n",
        "{input}\n",
        "\n",
        "### Response:\n",
        "{output}\n",
        "```\n",
        "\n",
        "##  Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class InstructionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for instruction fine-tuning.\n",
        "    \n",
        "    Format: instruction + input → response\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, data, tokenizer, max_length=256):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data: List of dicts with 'instruction', 'input', 'output'\n",
        "            tokenizer: Tokenizer\n",
        "            max_length: Max sequence length\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "    \n",
        "    def format_prompt(self, example):\n",
        "        \"\"\"Format instruction into prompt template.\"\"\"\n",
        "        instruction = example['instruction']\n",
        "        input_text = example.get('input', '')\n",
        "        output = example['output']\n",
        "        \n",
        "        prompt = f\"### Instruction:\\n{instruction}\\n\\n\"\n",
        "        if input_text:\n",
        "            prompt += f\"### Input:\\n{input_text}\\n\\n\"\n",
        "        prompt += f\"### Response:\\n{output}\"\n",
        "        \n",
        "        return prompt\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        example = self.data[idx]\n",
        "        \n",
        "        # Format full prompt\n",
        "        full_prompt = self.format_prompt(example)\n",
        "        \n",
        "        # Tokenize (simple char tokenization for demo)\n",
        "        token_ids = [ord(c) % 256 for c in full_prompt]\n",
        "        \n",
        "        # Truncate if too long\n",
        "        if len(token_ids) > self.max_length:\n",
        "            token_ids = token_ids[:self.max_length]\n",
        "        \n",
        "        # Create input/target pairs (shifted by 1)\n",
        "        input_ids = token_ids[:-1]\n",
        "        target_ids = token_ids[1:]\n",
        "        \n",
        "        # Pad\n",
        "        padding_len = self.max_length - 1 - len(input_ids)\n",
        "        if padding_len > 0:\n",
        "            input_ids = input_ids + [0] * padding_len\n",
        "            target_ids = target_ids + [0] * padding_len\n",
        "        \n",
        "        return {\n",
        "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
        "            'target_ids': torch.tensor(target_ids, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "def train_instruction_model(model, train_loader, val_loader, optimizer, device, num_epochs):\n",
        "    \"\"\"\n",
        "    Training loop for instruction fine-tuning.\n",
        "    \n",
        "    Uses next-token prediction loss.\n",
        "    \"\"\"\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        \n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            target_ids = batch['target_ids'].to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass\n",
        "            logits = model(input_ids)\n",
        "            \n",
        "            # Calculate loss (ignore padding tokens)\n",
        "            loss = F.cross_entropy(\n",
        "                logits.view(-1, logits.size(-1)),\n",
        "                target_ids.view(-1),\n",
        "                ignore_index=0  # Ignore padding\n",
        "            )\n",
        "            \n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "        \n",
        "        train_loss = total_loss / len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "        \n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                target_ids = batch['target_ids'].to(device)\n",
        "                \n",
        "                logits = model(input_ids)\n",
        "                loss = F.cross_entropy(\n",
        "                    logits.view(-1, logits.size(-1)),\n",
        "                    target_ids.view(-1),\n",
        "                    ignore_index=0\n",
        "                )\n",
        "                val_loss += loss.item()\n",
        "        \n",
        "        val_loss = val_loss / len(val_loader)\n",
        "        val_losses.append(val_loss)\n",
        "        \n",
        "        # Calculate perplexity\n",
        "        train_ppl = torch.exp(torch.tensor(train_loss))\n",
        "        val_ppl = torch.exp(torch.tensor(val_loss))\n",
        "        \n",
        "        print(f\"\\nEpoch {epoch+1}:\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f}, Perplexity: {train_ppl:.2f}\")\n",
        "        print(f\"  Val Loss: {val_loss:.4f}, Perplexity: {val_ppl:.2f}\")\n",
        "    \n",
        "    return train_losses, val_losses\n",
        "\n",
        "\n",
        "#  Example: Instruction fine-tuning\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"INSTRUCTION FINE-TUNING EXAMPLE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Sample instruction dataset\n",
        "instruction_data = [\n",
        "    {\n",
        "        \"instruction\": \"What is AI?\",\n",
        "        \"input\": \"\",\n",
        "        \"output\": \"AI is artificial intelligence.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Translate to Spanish:\",\n",
        "        \"input\": \"Hello\",\n",
        "        \"output\": \"Hola\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"What is 2+2?\",\n",
        "        \"input\": \"\",\n",
        "        \"output\": \"2+2 equals 4.\"\n",
        "    },\n",
        "]\n",
        "\n",
        "print(f\"\\nInstruction dataset created:\")\n",
        "print(f\"  Total examples: {len(instruction_data)}\")\n",
        "print(f\"\\nExample:\")\n",
        "print(json.dumps(instruction_data[0], indent=2))\n",
        "\n",
        "# Create dataset\n",
        "inst_dataset = InstructionDataset(instruction_data, tokenizer, max_length=256)\n",
        "\n",
        "# Create dataloader\n",
        "inst_loader = DataLoader(inst_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "print(f\"\\nDataLoader created with {len(inst_loader)} batches\")\n",
        "\n",
        "# Use same GPT model for instruction tuning\n",
        "print(f\"\\nUsing pretrained GPT model...\")\n",
        "\n",
        "# Setup optimizer (very low LR for instruction fine-tuning)\n",
        "inst_optimizer = torch.optim.AdamW(base_gpt.parameters(), lr=5e-5, weight_decay=0.01)\n",
        "\n",
        "# Train\n",
        "print(f\"\\nStarting instruction fine-tuning...\")\n",
        "train_losses, val_losses = train_instruction_model(\n",
        "    base_gpt, inst_loader, inst_loader, inst_optimizer, device, num_epochs=2\n",
        ")\n",
        "\n",
        "print(f\"\\n Instruction fine-tuning completed!\")\n",
        "\n",
        "# Test generation\n",
        "print(f\"\\n\" + \"=\" * 60)\n",
        "print(\"TEST GENERATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "test_instruction = \"What is\"\n",
        "print(f\"\\nInstruction: {test_instruction}\")\n",
        "print(f\"Response: \", end=\"\")\n",
        "\n",
        "# Simple generation (greedy)\n",
        "base_gpt.eval()\n",
        "test_tokens = [ord(c) % 256 for c in test_instruction]\n",
        "generated = torch.tensor([test_tokens], dtype=torch.long).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for _ in range(20):\n",
        "        if generated.size(1) >= base_gpt.max_seq_len:\n",
        "            generated = generated[:, -base_gpt.max_seq_len:]\n",
        "        \n",
        "        logits = base_gpt(generated)\n",
        "        next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
        "        generated = torch.cat([generated, next_token], dim=1)\n",
        "\n",
        "# Decode\n",
        "generated_text = ''.join([chr(int(t) % 128) for t in generated[0]])\n",
        "print(generated_text)\n",
        "\n",
        "print(f\"\\n Generation completed!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
